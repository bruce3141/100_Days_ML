# Day 1:
Compared hyper-parameter optimization performance for a simple SVR (over C and gamma) using HyperOpt and a random search, surprisingly, HyperOpt seems to overfit compared to a random search, still working on making this a legit apples - apples comparison.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day_1.ipynb


# Day 2:
More work on hyper-param tuning:  used a grid search to map out SVR regressor performance as a function of the hyper-parameters: C and gamma. Compared final results with HyperOpt.  Great visualization tool (but slow) for mapping out global performance.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day_2.ipynb


# Day 3:
Day-3: Completed a Genetic Algorithm that optimizes ML hyper-parameters (simple SVR over C and gamma).  Wow, can really see why Bayes optimizers are so popular, the GA is crazy expensive. The “cool-algo” of 90’s is perhaps not the best tool for the job.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day-3.py


# Day 4:
Day-4: More work using the Genetic Algorithm that optimizes ML hyper-parameters (simple SVR over C and gamma).  Did a grid search to map out the hyperparameter space for C and gamma.  Originally used a 100x100 search grid but had to terminate program, scaled back to 20x20 and this even took 5 hours!  Oh yea, by the way, here's all my code: https://github.com/bruce3141/100_days_ML

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day-4.py


# Day 5:
Started reading a great book (at the recommendation of my brother): Blondie24 (https://www.amazon.com/Blondie24-Playing-Kaufmann-Artificial-Intelligence/dp/1558607838). Completed webinars/tutorials by @LucenaResearch, @sirajraval, and @deeplizard on Reinforcement Learning. Installed OpenAI Gym and made progress on the Frozen Lake problem. Tested a custom Keras regression metric: RMSE.

**Link:** https://github.com/bruce3141/100_days_ML

# Day-6:
Day-6: Early OOS backtest from my quant trading code looks promising (haha, my hobby). Learning from Jedi Master: @VolatilityQ. Also upgraded tools to OSX Mohave and Python 3.7. Watched latest webinars on Data Science by @jstauth and @deeplizard, super cool.

**Link:** https://github.com/bruce3141/100_days_ML

# Day-7:
Completed additional work using Hyperopt (a Bayesian parameter tuning framework) https://hyperopt.github.io/hyperopt/. Helped to refine the architecture for an MLP for 2-d data processing. Hyperopt saves a ton of time, I wrote my own optimizer because I thought I could do better but you end up chasing your tail in a lot of these cases, still learned a lot though! Happy with intermediate results.

**Link:** https://github.com/bruce3141/100_days_ML

# Day-8:
More progress with the Bayesian parameter tuning framework (https://hyperopt.github.io/hyperopt/) as applied to my MLP architecture for 2-d data processing.  Interesting results so far.

**Link:** https://github.com/bruce3141/100_days_ML

# Day-9:
More work on hyperparameter tuning: got several new cases running now, definitely can take a while, will check back in morning. In the meantime examining log-loss convergence and making correspondence with network performance.

**Link:** https://github.com/bruce3141/100_days_ML

# Day-10:
More work on hyperparameter tuning for my application: Jupyter notebooks keep crashing with "out of memory" errors, a bit frustrating since each calc can take > 2hrs.  Had to scale back sampling, need to het some GPUs running!

**Link:** https://github.com/bruce3141/100_days_ML

# Day-11:


**Link:** https://github.com/bruce3141/100_days_ML

# Day-12:


**Link:** https://github.com/bruce3141/100_days_ML

# Day-13:


**Link:** https://github.com/bruce3141/100_days_ML
