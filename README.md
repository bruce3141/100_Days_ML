# Day 1:
Compared hyper-parameter optimization performance for a simple SVR (over C and gamma) using HyperOpt and a random search, surprisingly, HyperOpt seems to overfit compared to a random search, still working on making this a legit apples - apples comparison.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day_1.ipynb


# Day 2:
More work on hyper-param tuning:  used a grid search to map out SVR regressor performance as a function of the hyper-parameters: C and gamma. Compared final results with HyperOpt.  Great visualization tool (but slow) for mapping out global performance.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day_2.ipynb


# Day 3:
Day-3: Completed a Genetic Algorithm that optimizes ML hyper-parameters (simple SVR over C and gamma).  Wow, can really see why Bayes optimizers are so popular, the GA is crazy expensive. The “cool-algo” of 90’s is perhaps not the best tool for the job.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day-3.py


# Day 4:
Day-4: More work using the Genetic Algorithm that optimizes ML hyper-parameters (simple SVR over C and gamma).  Did a grid search to map out the hyperparameter space for C and gamma.  Originally used a 100x100 search grid but had to terminate program, scaled back to 20x20 and this even took 5 hours!  Oh yea, by the way, here's all my code: https://github.com/bruce3141/100_days_ML

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day-4.py


# Day 5:
Started reading a great book (at the recommendation of my brother): Blondie24 (https://www.amazon.com/Blondie24-Playing-Kaufmann-Artificial-Intelligence/dp/1558607838). Completed webinars/tutorials by @LucenaResearch, @sirajraval, and @deeplizard on Reinforcement Learning. Installed OpenAI Gym and made progress on the Frozen Lake problem. Tested a custom Keras regression metric: RMSE.

**Link:** https://github.com/bruce3141/100_days_ML

# Day-6:

**Link:**
