# Day 1:

Compared hyper-parameter optimization performance for a simple SVR (over C and gamma) using HyperOpt and a random search, surprisingly, HyperOpt seems to overfit compared to a random search, still working on making this a legit apples - apples comparison.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day_1.ipynb


# Day 2:

More work on hyper-param tuning:  used a grid search to map out SVR regressor performance as a function of the hyper-parameters: C and gamma. Compared final results with HyperOpt.  Great visualization tool (but slow) for mapping out global performance.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day_2.ipynb


# Day 3:

Day-3: Completed a Genetic Algorithm that optimizes ML hyper-parameters (simple SVR over C and gamma).  Wow, can really see why Bayes optimizers are so popular, the GA is crazy expensive. The “cool-algo” of 90’s is perhaps not the best tool for the job.

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day-3.py


# Day 4:

Day-4: More work using the Genetic Algorithm that optimizes ML hyper-parameters (simple SVR over C and gamma).  Did a grid search to map out the hyperparameter space for C and gamma.  Originally used a 100x100 search grid but had to terminate program, scaled back to 5x5 and this even took 5 hours!

**Link:** https://github.com/bruce3141/100_days_ML/blob/master/day-4.py


# Day 5:

Day-5:

**Link:**
